<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ImOV3D_NeurIPS2024</title>
  <link rel="icon" type="image/x-icon" href="static/images/ImOV3D_Icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .im-color { color: #e23e57; }
    .o-color { color: #f07b3f; }
    .v-color { color: #00b8a9; }
    .d-color { color: #3490de; }
    .three-color { color: #3490de; } /* å‡è®¾3Dä¸­çš„3å’ŒDéƒ½æ˜¯è“è‰² */
</style>
<style>
  .eql-cntrb {
      display: inline; /* æˆ–è€…ä½¿ç”¨ display: inline-block; */
      vertical-align: top; /* ç¡®ä¿ä¸Šæ ‡å¯¹é½ */
      margin-right: 10px; /* ç»™ä¸¤ä¸ªå…ƒç´ ä¹‹é—´ç•™ç‚¹ç©ºé—´ */
  }
  small {
      display: inline; /* ç¡®ä¿<small>æ ‡ç­¾ä¸ä¼šæ¢è¡Œ */
  }
</style>


<style>
  .link-block a {
      text-decoration: none;
      border: none;
      background: none;
      font-weight: bold;
      color: black;
      display: inline-block; /* ç¡®ä¿é“¾æ¥æ˜¯å†…è”å—çº§å…ƒç´  */
  }

  .link-block .icon {
      font-size: 24px; /* è®¾ç½®å›¾æ ‡å¤§å° */
      margin-right: 8px; /* å›¾æ ‡å’Œæ–‡æœ¬ä¹‹é—´çš„é—´è· */
  }

  .link-block .icon i {
      font-size: inherit; /* ç»§æ‰¿çˆ¶å…ƒç´ çš„å­—ä½“å¤§å° */
  }

  .link-block span {
      font-size: 16px; /* è®¾ç½®æ–‡æœ¬å¤§å° */
      vertical-align: middle; /* å‚ç›´å±…ä¸­æ–‡æœ¬å’Œå›¾æ ‡ */
  }
</style>

<style>
  .content pre {
    background-color: #f5f5f5; /* èƒŒæ™¯é¢œè‰² */
    border: 1px solid #ccc; /* è¾¹æ¡† */
    padding: 10px; /* å†…è¾¹è· */
    white-space: pre-wrap; /* ä¿æŒç©ºæ ¼å’Œæ¢è¡Œï¼ŒåŒæ—¶å…è®¸æ–‡æœ¬è‡ªåŠ¨æ¢è¡Œ */
  }
  .content code {
    font-family: monospace; /* ç­‰å®½å­—ä½“ */
    font-size: 0.9em; /* å­—ä½“å¤§å° */
  }
</style>

<style>
  .link-container {
    display: flex;
    justify-content: center;
    gap: 15px; /* æŒ‰é’®ä¹‹é—´çš„é—´è· */
    margin-top: 20px; /* ä¸ä¸Šæ–¹å†…å®¹çš„é—´è· */
  }
  .link-block {
    display: flex;
    align-items: center;
    justify-content: center;
    background-color: #333; /* èƒŒæ™¯è‰² */
    color: white;
    padding: 10px 20px;
    border-radius: 25px; /* åœ†è§’ */
    text-decoration: none;
    font-size: 16px; /* æ–‡å­—å¤§å° */
    transition: background-color 0.3s ease; /* æ‚¬åœè¿‡æ¸¡æ•ˆæœ */
  }
  .link-block:hover {
    background-color: #797777; /* æ‚¬åœæ—¶å˜äº® */
  }
  .link-block .icon {
    margin-right: 8px; /* å›¾æ ‡ä¸æ–‡å­—ä¹‹é—´çš„é—´è· */
  }
  .link-block .icon i {
    font-size: 18px; /* å›¾æ ‡å¤§å° */
  }
</style>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title">ImOV3D: Learning <u>O</u>pen-<u>V</u>ocabulary Point Clouds <u>3D</u> Object Detection from Only 2D <u>Im</u>ages
</h1> -->
            <h1 class="title is-1 publication-title" style="font-size: 40px;">
              <span class="im-color">Im</span><span class="o-color">O</span><span class="v-color">V</span><span class="three-color">3D</span>: Learning 
              <span class="o-color">O</span>pen-<span class="v-color">V</span>ocabulary Point Clouds 
              <span class="three-color">3D</span> Object Detection from Only 2D 
              <span class="im-color">Im</span>ages
            </h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yangtiming.github.io" target="_blank">Timing Yang</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://x.com/averyjuuu0213" target="_blank">Yuanliang Ju</a><sup>1,2*</sup>,</span>
                  <span class="author-block">
                    <a href="https://ericyi.github.io/" target="_blank">Li Yi</a><sup>2,3,1 &#8224;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Shanghai Qi Zhi Institute, <sup>2</sup> IIIS, Tsinghua University, <sup>3</sup> Shanghai AI Lab
                    <br><b >ğŸ‡¨ğŸ‡¦ NeurIPS 2024 ğŸ”¥</b></span><br>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>&#8224;</sup>Corresponding Author</small></span> -->
                    <span class="eql-cntrb">
                      <small>
                          <sup>*</sup>Equal Contribution
                      </small>
                  </span>
                  <span class="eql-cntrb">
                      <small>
                          <sup>&#8224;</sup>Corresponding Author
                      </small>
                  </span>
                    
                  </div>
                  <div class="link-container">
                    <a href="https://arxiv.org/pdf/2410.24001v1" target="_blank" class="link-block">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  
                    <a href="https://drive.google.com/file/d/1azNJz1y2zi_eZrvCrY5zkJ9lDgjPA7O7/view?usp=sharing" target="_blank" class="link-block">
                      <span class="icon">
                        <i class="fas fa-file-alt"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  
                    <a href="https://github.com/yangtiming/ImOV3D" target="_blank" class="link-block">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>

                    <a href="https://x.com/AveryJuuu0213/status/1866901396077084766" target="_blank" class="link-block">
                      <span class="icon">
                        <i class="fab fa-twitter"></i>
                      </span>
                      <span>X</span>
                    </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="Abstract" style="margin-top: 5px;">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered">Abstract</h2>
    <div class="content has-text-justified is-centered" style="padding: 20px;">
      <p>
        Open-vocabulary 3D object detection (OV-3Det) aims to generalize beyond the limited number of base categories labeled during the training phase. The biggest bottleneck is the scarcity of annotated 3D data, whereas 2D image datasets are abundant and richly annotated. Consequently, it is intuitive to leverage the wealth of annotations in 2D images to alleviate the inherent data scarcity in OV-3Det. In this paper, we push the task setup to its limits by exploring the potential of using solely 2D images to learn OV-3Det. The major challenges for this setup is the modality gap between training image and testing point cloud, which prevents effective integration of 2D knowledge into OV-3Det.
      </p>
      <!-- åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡ -->
      <img src="static/images/t3.png" alt="Description of the image" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
      <!-- <figure>
        <img src="static/images/t3.png" alt="Description of the image" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
        <figcaption>è¿™é‡Œæ˜¯å›¾ç‰‡çš„æ ‡é¢˜</figcaption>
      </figure> -->
      <p>
        To address this challenge, we propose a novel framework just have <a href=""></a><b><span class="im-color">Im</span><span class="o-color">O</span><span class="v-color">V</span><span class="three-color">3D</span></b> to leverage pseudo multimodal representation containing both images and point clouds (PC) to close the modality gap. The key of ImOV3D lies in flexible modality conversion where 2D images can be lifted into 3D using monocular depth estimation and can also be derived from 3D scenes through rendering. This allows unifying both training images and testing point cloud into a common image-PC representation, encompassing a wealth of 2D semantic information and also incorporating the depth and structural characteristics of 3D spatial data. We carefully conduct such conversion to minimize the domain gap between training and test cases. Extensive experiments on two benchmark datasets, SUNRGBD and ScanNet, show that ImOV3D significantly outperforms existing methods, even in the absence of ground truth 3D training data. With the inclusion of a minimal amount of real 3D data for fine-tuning, the performance also significantly surpasses previous state-of-the-art.
      </p>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">Our Pipeline</h2>
      <div class="overview-video" style="max-width: 77%; margin: 0 auto; text-align: center;">
        <!-- Embed YouTube video -->
        <iframe 
          src="https://www.youtube.com/embed/R052kov-1l0" 
          title="YouTube video player" 
          width="100%" 
          height="498" 
          style="border: none; display: block; margin: 0 auto;" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen>
        </iframe>
        <p style="text-align: justify; line-height: 1.6;"> Overview of <b><span class="im-color">Im</span><span class="o-color">O</span><span class="v-color">V</span><span class="three-color">3D</span></b>: Our model takes 2D images as input and puts them into the Pseudo
          3D Annotation Generator to produce pseudo annotations. These 2D images are also fed into the
          Point Cloud Lifting Module to generate pseudo point clouds. Subsequently, using the Point Cloud
          Renderer, these pseudo point clouds are rendered into pseudo images, which then get processed by a
          2D open vocabulary detector to detect 2D proposals and transfer the 2D semantic information to 3D
          space. Armed with pseudo point clouds, annotations, and pseudo images data, we proceed to train a
          multimodal 3D detector.</p>
      </div>
    </div>
  </div>
</section>


<!-- Overview image section -->  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">3D Data Revision Module</h2>
      <div class="overview-image">
        <!-- Adjust the image size by setting width -->
        <img src="static/images/77.png" alt="Overview image" style="width: 77%; margin: 0 auto; display: block;"/>
        <p style="text-align: justify; line-height: 1.6; max-width: 77%; margin: 0 auto;">
          <b style="color: #b83b5e;">(a)</b> The rotation correction module involves processing an RGB image through a Normal Estimator to generate a normal map. This map then helps extract a horizontal surface mask for identifying horizontal point clouds, from which normal vectors are obtained. These vectors are aligned with the Z-axis to compute the rotation matrix. <b style="color: #b83b5e;">(b)</b> In the 3D box filtering module, prompts related to object dimensions are first provided to GPT-4 to determine the mean size for each category. This mean size is then used to filter out boxes that do not meet the threshold criteria.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- End overview image section -->




<!-- Image carousel -->
<!-- <section class="hero is-small">
  <h2 class="title is-3 has-text-centered">Visualization of Data Generation</h2>
  <div style="max-width: 70%; margin: 0 auto;">
  <h2 class="subtitle has-text-centered" style="font-size: 18px;">è°ƒæ•´å­—ä½“å¤§å° -->
    <!-- We use the first image from two scenes in the ScanNet dataset as examples to demonstrate the entire process of Data Generation. This explains how 2D data is converted into 3D data, and how 3D data is then transformed back into 2D pseudo data for multimodal training.
  </h2>
</div>

  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          Your image here -->
          <!-- <img src="static/images/demo_0000.png" alt="Overview image" style="width: 70%; margin: 0 auto; display: block;"/>
          <div style="max-width: 70%; margin: 0 auto;">
            <h2 class="subtitle has-text-centered">
              Scene_0000_01 Visualization
            </h2>
          </div>
        </div>

        <div class="item">
           Your image here -->
          <!-- <img src="static/images/demo_00012.png" alt="Overview image" style="width: 70%; margin: 0 auto; display: block;"/>
          <div style="max-width: 70%; margin: 0 auto;">
            <h2 class="subtitle has-text-centered">
              Scene_0012_01 Visualization
            </h2> -->
          <!-- </div> -->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> --> 
<!-- End image carousel -->


<section class="section hero is-light">
  <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">Performance Comparison</h2>

<style>
  table {
    width: 70%; /* è¡¨æ ¼å®½åº¦ */
    margin: auto;
    border-collapse: collapse;
  }
  th, td {
    text-align: center;
    vertical-align: middle;
    padding: 8px;
    font-size: 12px;
  }
  th {
    background-color: #f2f2f2;
    border-bottom: 2px solid black; /* ä¸‰çº¿è¡¨ä¸Šæ¨ªçº¿ */
  }
  tr {
    border-bottom: 1px solid black; /* ä¸‰çº¿è¡¨ä¸­é—´æ¨ªçº¿ */
  }
  table tbody tr:last-child {
    border-bottom: 2px solid black; /* ä¸‰çº¿è¡¨ä¸‹æ¨ªçº¿ */
  }
  .highlight {
    background-color: #d5f5d3; /* é«˜äº®é¢œè‰² */
  }
  .increase {
    color: red;
  }
</style>

<table>
  <caption>Results from the <b style="color: #b83b5e;">Pretraining stage</b> comparison experiments on SUNRGBD and ScanNet, ImOV3D only require point clouds input.</caption>
  <thead>
    <tr>
      <th style="text-align:center; vertical-align:middle;">Stage</th>
      <th style="text-align:center; vertical-align:middle;">Data Type</th>
      <th style="text-align:center; vertical-align:middle;">Method</th>
      <th style="text-align:center; vertical-align:middle;">Input</th>
      <th style="text-align:center; vertical-align:middle;">Training Strategy</th>
      <th style="text-align:center; vertical-align:middle;">SUNRGBD mAP@0.25</th>
      <th style="text-align:center; vertical-align:middle;">ScanNet mAP@0.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:center; vertical-align:middle;" rowspan="4">Pre-training</td>
      <td style="text-align:center; vertical-align:middle;" rowspan="4">Pseudo Data</td>
      <td style="text-align:center; vertical-align:middle;">OV-VoteNet</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud</td>
      <td style="text-align:center; vertical-align:middle;">One-Stage</td>
      <td style="text-align:center; vertical-align:middle;">5.18</td>
      <td style="text-align:center; vertical-align:middle;">5.86</td>
    </tr>
    <tr>
      <td style="text-align:center; vertical-align:middle;">OV-3DETR</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud</td>
      <td style="text-align:center; vertical-align:middle;">One-Stage</td>
      <td style="text-align:center; vertical-align:middle;">5.24</td>
      <td style="text-align:center; vertical-align:middle;">5.30</td>
    </tr>
    <tr>
      <td style="text-align:center; vertical-align:middle;">OV-3DET</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud + Image</td>
      <td style="text-align:center; vertical-align:middle;">Two-Stage</td>
      <td style="text-align:center; vertical-align:middle;">5.47</td>
      <td style="text-align:center; vertical-align:middle;">5.69</td>
    </tr>
    <tr class="highlight">
      <td style="text-align:center; vertical-align:middle;">Ours</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud</td>
      <td style="text-align:center; vertical-align:middle;">One-Stage</td>
      <td style="text-align:center; vertical-align:middle;"><b>12.61</b> <span class="increase"><b>â†‘ 7.14</b></span></td>
      <td style="text-align:center; vertical-align:middle;"><b>12.64</b> <span class="increase"><b>â†‘ 6.78</b></span></td>
    </tr>
  </tbody>
</table>

<br><br>
<table>
  <caption>Results from the <b style="color: #b83b5e;">Adaptation stage</b> comparison experiments on SUNRGBD and ScanNet</caption>
  <thead>
    <tr>
      <th style="text-align:center; vertical-align:middle;">Stage</th>
      <th style="text-align:center; vertical-align:middle;">Method</th>
      <th style="text-align:center; vertical-align:middle;">Input</th>
      <th style="text-align:center; vertical-align:middle;">Training Strategy</th>
      <th style="text-align:center; vertical-align:middle;">SUNRGBD mAP@0.25</th>
      <th style="text-align:center; vertical-align:middle;">ScanNet mAP@0.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:center; vertical-align:middle;" rowspan="3">Adaptation</td>
      <td style="text-align:center; vertical-align:middle;">OV-3DET</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud + Image</td>
      <td style="text-align:center; vertical-align:middle;">Two-Stage</td>
      <td style="text-align:center; vertical-align:middle;">20.46</td>
      <td style="text-align:center; vertical-align:middle;">18.02</td>
    </tr>
    <tr>
      <td style="text-align:center; vertical-align:middle;">CoDA</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud</td>
      <td style="text-align:center; vertical-align:middle;">One-Stage</td>
      <td style="text-align:center; vertical-align:middle;">â€”</td>
      <td style="text-align:center; vertical-align:middle;">19.32</td>
    </tr>
    <tr class="highlight">
      <td style="text-align:center; vertical-align:middle;">Ours</td>
      <td style="text-align:center; vertical-align:middle;">Point Cloud</td>
      <td style="text-align:center; vertical-align:middle;">One-Stage</td>
      <td style="text-align:center; vertical-align:middle;"><b>22.53</b> <span class="increase"><b>â†‘ 2.07</b></span></td>
      <td style="text-align:center; vertical-align:middle;"><b>21.45</b> <span class="increase"><b>â†‘ 2.13</b></span></td>
    </tr>
  </tbody>
</table>

</section>


  
<!-- Overview image section -->  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">Analysis of Depth and Pseudo Images</h2>
      <div class="overview-image">
        <!-- Adjust the image size by setting width -->
        <img src="static/images/depth_color.png" alt="Overview image" style="width: 77%; margin: 0 auto; display: block;"/>
        <p style="text-align: justify; line-height: 1.6; max-width: 77%; margin: 20px auto; font-size: 1rem;">
          Generating Pseudo 2D data as an auxiliary part of 3D data is necessary. <b style="color: #b83b5e;">(a)</b> shows the original 2D RGB images, 
          <b style="color: #b83b5e;">(b)</b> shows the 2D depth maps with 2D OVDetector annotations, and <b style="color: #b83b5e;">(c)</b> shows the pseudo images with annotations 
          from a 2D OVDetector. It can be observed that the Pseudo Image captures more useful information, which enhances the 3D 
          detection performance.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- End overview image section -->



 
<!-- Overview image section -->  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">Analysis of Data Volume and Transferability</h2>
      <div class="overview-image">
        <!-- Adjust the image size by setting width -->
        <img src="static/images/vol_trans_higher_res.png" alt="Overview image" style="width: 77%; margin: 0 auto; display: block;"/>
        <p style="text-align: justify; line-height: 1.6; max-width: 77%; margin: 20px auto; font-size: 1rem;">
          <b style="color: #b83b5e;">(a)</b> and <b style="color: #b83b5e;">(b)</b> show the data volume ablation results. Under the adaptation setting, we used varying percentages of data volumes from 100% to 10% for training to determine the specific impact of data scarcity on model performance. It demonstrates that using 2D data for pretraining can help the model maintain high performance even with a small amount of 3D data. <b style="color: #b83b5e;">(c)</b> illustrates transferability ablation results. 2D to 3D knowledge transfer strategy not only improves the modelâ€™s understanding of 3D shapes and structures but also significantly enhances the modelâ€™s generalization ability and detection accuracy across different datasets.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered" style="font-size: 1.5rem;">The comparison of detection result visualizations</h2>
      <!-- <h2 class="subtitle has-text-centered" style="font-size: 18px;">
        During the pretraining stage, the visual comparison tested on the SUNRGBD/ScanNet dataset includes four columns: the first column shows the ground truth images, the second column displays the ground truth point clouds, the third column presents the detection results by OV-3DET, and the fourth column reveals the detection outcomes by ImOV3D.
      </h2> -->
      
      <div class="content has-text-centered" style="display: flex; justify-content: center; align-items: center; flex-wrap: wrap;">
        <div style="flex: 1; margin: 0 10px; text-align: center;">
          <img src="static/images/sun_vis_lower_dpi.png" alt="Overview image SUNRGBD" style="width: %; height: 300px; object-fit: cover;"/>
          <h2 class="subtitle has-text-centered" style="font-size: 18px; color: #b83b5e;">
            The visualizations of SUNRGBD
          </h2>
        </div>
        <div style="flex: 1; margin: 0 10px; text-align: center;">
          <img src="static/images/scan_vis_lower_dpi.png" alt="Overview image ScanNet" style="width: %; height: 300px; object-fit: cover;"/>
          <h2 class="subtitle has-text-centered" style="font-size: 18px; color: #b83b5e;">
            The visualizations of ScanNet
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Paper poster -->
<section class="section" id="Poster">
  <div class="container is-max-desktop content">
    <h2 class="title">Poster</h2>
    
    <!-- ç¡®ä¿iframeå±…ä¸­æ˜¾ç¤º -->
    <div class="content has-text-centered">
      <iframe src="static/pdfs/ImOV3D_Poster.pdf" width="89%" height="625"></iframe>
    </div>
    
  </div>
</section>
<!--End paper poster -->

<section class="section" id="Connect">
  <div class="container is-max-desktop content">
    <h2 class="title">Connect</h2>
    <div class="content">
      <p style="color: #b83b5e;">If you have any questions, please feel free to contact us:</p>
      <ul>
        <li><strong>Timing Yang:</strong> <a href="mailto:timingya@usc.edu">timingya@usc.edu</a></li>
        <li><strong>Yuanliang Ju:</strong> <a href="mailto:yuanliang.ju@mail.utoronto.ca">yuanliang.ju@mail.utoronto.ca</a></li>
      </ul>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code class="bibtex">@article{yang2024imov3d,
  title={ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from Only 2D Images},
  author={Yang, Timing and Ju, Yuanliang and Yi, Li},
  journal={NeurIPS 2024},
  year={2024}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
